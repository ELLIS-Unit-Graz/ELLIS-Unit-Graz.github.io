---
title: Workshops
nav:
  order: 4
  tooltip: ELLIS UnConference
---

# {% include icon.html icon="fa-solid fa-users" %}Energy-Efficient AI: Models, Algorithms, and Hardware for Sustainable Intelligence

As artificial intelligence systems grow in scale and capability, their energy demands are increasing at an unsustainable pace. This workshop addresses the urgent need for energy-efficient AI, a field that seeks to develop environmentally responsible and computationally efficient approaches to machine learning. The goal is to explore innovations that reduce energy consumption across the AI stack — from models and learning algorithms to hardware implementations — while maintaining or even enhancing performance. One source of inspiration for reaching this goal is the brain, which manages to provide intelligence with 20W. 

{% include section.html %}

# Call for Contributions

The workshop will be inclusive and accessible to a broad audience across machine learning, computer architecture, and systems engineering. Topics include but are not limited to: 

Model compression and pruning techniques that reduce inference and training costs
Quantization and low-precision arithmetic for efficient deployment
New learning algorithms designed for reduced computational and memory overhead
Architectures and model designs tailored for energy-aware operation
Neuromorphic computing and spiking neural networks inspired by biological efficiency
Hardware accelerators, including FPGAs, photonic processors, and neuromorphic chips, that offer novel energy-performance trade-offs

This workshop aims to bring together researchers and practitioners from diverse backgrounds to foster collaboration and cross-pollination of ideas in making AI more sustainable without compromising its transformative potential.
